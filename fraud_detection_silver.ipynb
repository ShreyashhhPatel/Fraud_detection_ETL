{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd49e012-9755-424f-8f5e-4e59acf8df57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"USE CATALOG workspace\")\n",
    "spark.sql(\"USE SCHEMA fraud_detection\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "567c6a45-0c50-4cda-a0cc-599a86f9d9cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "silver_customer_df = (\n",
    "    spark.table(\"bronze_customers\")\n",
    "    .select(\"customer_id\", \"country_code\")\n",
    "    .dropna(subset=[\"customer_id\"])\n",
    "    .dropDuplicates([\"customer_id\"])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb852db5-e4bb-44fd-891c-7e5e18fb27d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    silver_customer_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"silver_dim_customer\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01fe2b5f-667a-4f06-9454-cbd2081b2f23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>country_code</th></tr></thead><tbody><tr><td>C2317870228</td><td>TUR</td></tr><tr><td>C4603372195</td><td>REU</td></tr><tr><td>C1189806313</td><td>CAN</td></tr><tr><td>C9679880050</td><td>ESP</td></tr><tr><td>C2796449758</td><td>ESP</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "C2317870228",
         "TUR"
        ],
        [
         "C4603372195",
         "REU"
        ],
        [
         "C1189806313",
         "CAN"
        ],
        [
         "C9679880050",
         "ESP"
        ],
        [
         "C2796449758",
         "ESP"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "customer_id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "country_code",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 6
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country_code",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT COUNT(*) FROM silver_dim_customer;\n",
    "SELECT * FROM silver_dim_customer LIMIT 5;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f13c8c6-d89b-477a-b7db-ec8033b10d9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_country_df = (\n",
    "    spark.table(\"bronze_country_code\")\n",
    "    .select(\"country_code\")\n",
    "    .dropna()\n",
    "    .dropDuplicates()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1d70097-c222-47d4-8375-0a13bcfea601",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    silver_country_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"silver_dim_country\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53254a87-3517-4276-bdbd-574401227ee5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>country_code</th></tr></thead><tbody><tr><td>CAN</td></tr><tr><td>ITA</td></tr><tr><td>ESP</td></tr><tr><td>QAT</td></tr><tr><td>PSE</td></tr><tr><td>PER</td></tr><tr><td>TUR</td></tr><tr><td>RUS</td></tr><tr><td>FRA</td></tr><tr><td>KHM</td></tr><tr><td>PAN</td></tr><tr><td>REU</td></tr><tr><td>BRA</td></tr><tr><td>NGA</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "CAN"
        ],
        [
         "ITA"
        ],
        [
         "ESP"
        ],
        [
         "QAT"
        ],
        [
         "PSE"
        ],
        [
         "PER"
        ],
        [
         "TUR"
        ],
        [
         "RUS"
        ],
        [
         "FRA"
        ],
        [
         "KHM"
        ],
        [
         "PAN"
        ],
        [
         "REU"
        ],
        [
         "BRA"
        ],
        [
         "NGA"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "country_code",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 11
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "country_code",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT COUNT(*) FROM silver_dim_country;\n",
    "SELECT * FROM silver_dim_country;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "558cbe4f-535b-41c5-8b70-19699898665b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#------------silver_fact_transactions-----------------\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d44931e-5956-44f9-9618-0dae0b12fe7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "silver_txn_df = (\n",
    "    spark.table(\"bronze_transactions\")\n",
    "    .select(\n",
    "        col(\"id\").alias(\"transaction_id\"),\n",
    "        \"step\",\n",
    "        \"type\",\n",
    "        col(\"amount\").cast(\"double\"),\n",
    "        col(\"nameOrig\").alias(\"customer_id\"),\n",
    "        col(\"nameDest\").alias(\"merchant_id\"),\n",
    "        col(\"oldBalanceOrig\").cast(\"double\"),\n",
    "        col(\"newBalanceOrig\").cast(\"double\"),\n",
    "        col(\"oldBalanceDest\").cast(\"double\"),\n",
    "        col(\"newBalanceDest\").cast(\"double\"),\n",
    "        \"countryOrig\",\n",
    "        \"countryDest\",\n",
    "        \"ingest_time\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "625a1e81-856a-41d2-b64d-d4f6069f072e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    silver_txn_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"silver_fact_transactions\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7319871e-7e00-4b44-b49b-e00f678f4928",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>transaction_id</th><th>step</th><th>type</th><th>amount</th><th>customer_id</th><th>merchant_id</th><th>oldBalanceOrig</th><th>newBalanceOrig</th><th>oldBalanceDest</th><th>newBalanceDest</th><th>countryOrig</th><th>countryDest</th><th>ingest_time</th></tr></thead><tbody><tr><td>94a0eaf0-3b62-460c-93c6-72e40bb11974</td><td>1</td><td>CASH_IN</td><td>78994.9</td><td>C5669199819</td><td>M6571322302</td><td>130484.07</td><td>209478.97</td><td>0.0</td><td>0.0</td><td>FRA</td><td>ITA</td><td>2026-02-08T21:37:25.820Z</td></tr><tr><td>6aa1b852-8aee-4e7c-af41-53561512a79d</td><td>1</td><td>CASH_IN</td><td>130967.02</td><td>C0165182576</td><td>M8239546933</td><td>35.88</td><td>131002.89</td><td>0.0</td><td>0.0</td><td>REU</td><td>PER</td><td>2026-02-08T21:37:25.820Z</td></tr><tr><td>1380fad7-5575-4f23-81d7-c7cab1c03458</td><td>1</td><td>CASH_IN</td><td>172473.56</td><td>C4621556856</td><td>M1469655819</td><td>52.79</td><td>172526.35</td><td>8022.82</td><td>8022.82</td><td>NGA</td><td>BRA</td><td>2026-02-08T21:37:25.820Z</td></tr><tr><td>2600ef3a-af26-410f-b97f-41bbbecbff4d</td><td>1</td><td>CASH_IN</td><td>284398.59</td><td>C7349398929</td><td>M1719835988</td><td>134044.23</td><td>418442.83</td><td>10058.01</td><td>10058.01</td><td>ESP</td><td>TUR</td><td>2026-02-08T21:37:25.820Z</td></tr><tr><td>5ed03836-07a9-4cc6-b332-a08225b3dada</td><td>2</td><td>CASH_IN</td><td>42972.58</td><td>C9441668199</td><td>M6087015760</td><td>447311.47</td><td>490284.05</td><td>0.0</td><td>0.0</td><td>FRA</td><td>CAN</td><td>2026-02-08T21:37:25.820Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "94a0eaf0-3b62-460c-93c6-72e40bb11974",
         1,
         "CASH_IN",
         78994.9,
         "C5669199819",
         "M6571322302",
         130484.07,
         209478.97,
         0.0,
         0.0,
         "FRA",
         "ITA",
         "2026-02-08T21:37:25.820Z"
        ],
        [
         "6aa1b852-8aee-4e7c-af41-53561512a79d",
         1,
         "CASH_IN",
         130967.02,
         "C0165182576",
         "M8239546933",
         35.88,
         131002.89,
         0.0,
         0.0,
         "REU",
         "PER",
         "2026-02-08T21:37:25.820Z"
        ],
        [
         "1380fad7-5575-4f23-81d7-c7cab1c03458",
         1,
         "CASH_IN",
         172473.56,
         "C4621556856",
         "M1469655819",
         52.79,
         172526.35,
         8022.82,
         8022.82,
         "NGA",
         "BRA",
         "2026-02-08T21:37:25.820Z"
        ],
        [
         "2600ef3a-af26-410f-b97f-41bbbecbff4d",
         1,
         "CASH_IN",
         284398.59,
         "C7349398929",
         "M1719835988",
         134044.23,
         418442.83,
         10058.01,
         10058.01,
         "ESP",
         "TUR",
         "2026-02-08T21:37:25.820Z"
        ],
        [
         "5ed03836-07a9-4cc6-b332-a08225b3dada",
         2,
         "CASH_IN",
         42972.58,
         "C9441668199",
         "M6087015760",
         447311.47,
         490284.05,
         0.0,
         0.0,
         "FRA",
         "CAN",
         "2026-02-08T21:37:25.820Z"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "transaction_id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "step",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "type",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "amount",
            "nullable": true,
            "type": "double"
           },
           {
            "metadata": {},
            "name": "customer_id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "merchant_id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "oldBalanceOrig",
            "nullable": true,
            "type": "double"
           },
           {
            "metadata": {},
            "name": "newBalanceOrig",
            "nullable": true,
            "type": "double"
           },
           {
            "metadata": {},
            "name": "oldBalanceDest",
            "nullable": true,
            "type": "double"
           },
           {
            "metadata": {},
            "name": "newBalanceDest",
            "nullable": true,
            "type": "double"
           },
           {
            "metadata": {},
            "name": "countryOrig",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "countryDest",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "ingest_time",
            "nullable": true,
            "type": "timestamp"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 16
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "transaction_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "step",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "amount",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "merchant_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "oldBalanceOrig",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "newBalanceOrig",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "oldBalanceDest",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "newBalanceDest",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "countryOrig",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "countryDest",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ingest_time",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT COUNT(*) FROM silver_fact_transactions;\n",
    "SELECT * FROM silver_fact_transactions LIMIT 5;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a559b1a9-b238-46f6-bde3-374b08cfdb43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#-------Silver_fact_fraud-----------\n",
    "#\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d51112d-7051-4f0e-9886-4b9ae4b09af2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_fraud_df = (\n",
    "    spark.table(\"bronze_fraud_report\")\n",
    "    .select(\n",
    "        col(\"id\").alias(\"transaction_id\"),\n",
    "        col(\"fraud_flag\").cast(\"int\")\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "183bef0f-0b17-415a-8292-aaadf06cdcf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    silver_fraud_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"silver_fact_fraud\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df172c4a-9503-4908-b650-f5a81a9fd0f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>fraud_flag</th><th>COUNT(*)</th></tr></thead><tbody><tr><td>1</td><td>12380</td></tr><tr><td>0</td><td>5430616</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         12380
        ],
        [
         0,
         5430616
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "fraud_flag",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "COUNT(*)",
            "nullable": false,
            "type": "long"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 21
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "fraud_flag",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "COUNT(*)",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT fraud_flag, COUNT(*) \n",
    "FROM silver_fact_fraud\n",
    "GROUP BY fraud_flag;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74534041-b76e-45b8-b515-443205588771",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>database</th><th>tableName</th><th>isTemporary</th></tr></thead><tbody><tr><td>fraud_detection</td><td>bronze_country_code</td><td>false</td></tr><tr><td>fraud_detection</td><td>bronze_customers</td><td>false</td></tr><tr><td>fraud_detection</td><td>bronze_fraud_report</td><td>false</td></tr><tr><td>fraud_detection</td><td>bronze_transactions</td><td>false</td></tr><tr><td>fraud_detection</td><td>silver_dim_country</td><td>false</td></tr><tr><td>fraud_detection</td><td>silver_dim_customer</td><td>false</td></tr><tr><td>fraud_detection</td><td>silver_fact_fraud</td><td>false</td></tr><tr><td>fraud_detection</td><td>silver_fact_transactions</td><td>false</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "fraud_detection",
         "bronze_country_code",
         false
        ],
        [
         "fraud_detection",
         "bronze_customers",
         false
        ],
        [
         "fraud_detection",
         "bronze_fraud_report",
         false
        ],
        [
         "fraud_detection",
         "bronze_transactions",
         false
        ],
        [
         "fraud_detection",
         "silver_dim_country",
         false
        ],
        [
         "fraud_detection",
         "silver_dim_customer",
         false
        ],
        [
         "fraud_detection",
         "silver_fact_fraud",
         false
        ],
        [
         "fraud_detection",
         "silver_fact_transactions",
         false
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "database",
            "nullable": false,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "tableName",
            "nullable": false,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "isTemporary",
            "nullable": false,
            "type": "boolean"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 22
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "database",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tableName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isTemporary",
         "type": "\"boolean\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SHOW TABLES;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83093873-89ee-4c11-9834-eb5110b96cf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------\n",
    "#\n",
    "#---------Silver_Normalization--------------\n",
    "#\n",
    "#-------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ec0be51-74b9-4fbc-83f6-4f27c32a28ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_dim_customer_df = (\n",
    "    spark.table(\"bronze_customers\")\n",
    "    .select(\n",
    "        \"customer_id\",\n",
    "        \"country_code\"\n",
    "    )\n",
    "    .dropna(subset=[\"customer_id\"])\n",
    "    .dropDuplicates([\"customer_id\"])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45d37067-9c53-485c-b83f-02b18212eb9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    silver_dim_customer_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"silver_dim_customer\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdb4443a-efd9-4f51-a344-e11d794f6d33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>country_code</th></tr></thead><tbody><tr><td>C2317870228</td><td>TUR</td></tr><tr><td>C4603372195</td><td>REU</td></tr><tr><td>C1189806313</td><td>CAN</td></tr><tr><td>C9679880050</td><td>ESP</td></tr><tr><td>C2796449758</td><td>ESP</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "C2317870228",
         "TUR"
        ],
        [
         "C4603372195",
         "REU"
        ],
        [
         "C1189806313",
         "CAN"
        ],
        [
         "C9679880050",
         "ESP"
        ],
        [
         "C2796449758",
         "ESP"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "customer_id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "country_code",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 28
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country_code",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT COUNT(*) FROM silver_dim_customer;\n",
    "SELECT * FROM silver_dim_customer LIMIT 5;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec09cbd6-1c64-4126-9110-a666ba317014",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_dim_country_df = (\n",
    "    spark.table(\"bronze_country_code\")\n",
    "    .select(\"country_code\")\n",
    "    .dropna()\n",
    "    .dropDuplicates()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b5bc878-f0ff-4731-ba9f-6529e0da789f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    silver_dim_country_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"silver_dim_country\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80f53a39-5af7-455b-b2c5-f9d74afdd747",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>country_code</th></tr></thead><tbody><tr><td>CAN</td></tr><tr><td>ITA</td></tr><tr><td>ESP</td></tr><tr><td>QAT</td></tr><tr><td>PSE</td></tr><tr><td>PER</td></tr><tr><td>TUR</td></tr><tr><td>RUS</td></tr><tr><td>FRA</td></tr><tr><td>KHM</td></tr><tr><td>PAN</td></tr><tr><td>REU</td></tr><tr><td>BRA</td></tr><tr><td>NGA</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "CAN"
        ],
        [
         "ITA"
        ],
        [
         "ESP"
        ],
        [
         "QAT"
        ],
        [
         "PSE"
        ],
        [
         "PER"
        ],
        [
         "TUR"
        ],
        [
         "RUS"
        ],
        [
         "FRA"
        ],
        [
         "KHM"
        ],
        [
         "PAN"
        ],
        [
         "REU"
        ],
        [
         "BRA"
        ],
        [
         "NGA"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "country_code",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 31
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "country_code",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT * FROM silver_dim_country;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bb8575c-271f-4521-81af-459868daad51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "silver_fact_transactions_df = (\n",
    "    spark.table(\"bronze_transactions\")\n",
    "    .select(\n",
    "        col(\"id\").alias(\"transaction_id\"),\n",
    "        col(\"step\").cast(\"int\"),\n",
    "        col(\"type\"),\n",
    "        col(\"amount\").cast(\"double\"),\n",
    "        col(\"nameOrig\").alias(\"customer_id\"),\n",
    "        col(\"nameDest\").alias(\"merchant_id\"),\n",
    "        col(\"oldBalanceOrig\").cast(\"double\"),\n",
    "        col(\"newBalanceOrig\").cast(\"double\"),\n",
    "        col(\"oldBalanceDest\").cast(\"double\"),\n",
    "        col(\"newBalanceDest\").cast(\"double\"),\n",
    "        col(\"countryOrig\"),\n",
    "        col(\"countryDest\"),\n",
    "        col(\"ingest_time\")\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07ed24e9-b91d-44a3-a3a6-3fa5986a8279",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    silver_fact_transactions_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(\"silver_fact_transactions\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1857fcf-dc18-4bbf-8a75-1a6db54f6d0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.empty-table+json": {
       "directive_name": "DropTable"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS silver_fact_transactions;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d32a0ea-e5d5-4a3b-b32a-6d129ea10a80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-8907920231679062>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_cell_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msql\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDESCRIBE silver_fact_transactions;\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mSELECT COUNT(*) FROM silver_fact_transactions;\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mSELECT * FROM silver_fact_transactions LIMIT 5;\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2541\u001B[0m, in \u001B[0;36mInteractiveShell.run_cell_magic\u001B[0;34m(self, magic_name, line, cell)\u001B[0m\n",
       "\u001B[1;32m   2539\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n",
       "\u001B[1;32m   2540\u001B[0m     args \u001B[38;5;241m=\u001B[39m (magic_arg_s, cell)\n",
       "\u001B[0;32m-> 2541\u001B[0m     result \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m   2543\u001B[0m \u001B[38;5;66;03m# The code below prevents the output from being displayed\u001B[39;00m\n",
       "\u001B[1;32m   2544\u001B[0m \u001B[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001B[39;00m\n",
       "\u001B[1;32m   2545\u001B[0m \u001B[38;5;66;03m# when the last Python token in the expression is a ';'.\u001B[39;00m\n",
       "\u001B[1;32m   2546\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(fn, magic\u001B[38;5;241m.\u001B[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001B[38;5;28;01mFalse\u001B[39;00m):\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:194\u001B[0m, in \u001B[0;36mSqlMagic.sql\u001B[0;34m(self, line, cell)\u001B[0m\n",
       "\u001B[1;32m    188\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    189\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\n",
       "\u001B[1;32m    190\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_FAILED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    191\u001B[0m         exceptionClassName\u001B[38;5;241m=\u001B[39me\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m,\n",
       "\u001B[1;32m    192\u001B[0m         sqlState\u001B[38;5;241m=\u001B[39msafe_call(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgetSqlState\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n",
       "\u001B[1;32m    193\u001B[0m         errorClass\u001B[38;5;241m=\u001B[39msafe_call(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgetCondition\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
       "\u001B[0;32m--> 194\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
       "\u001B[1;32m    196\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_SUCCEEDED\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m    197\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:187\u001B[0m, in \u001B[0;36mSqlMagic.sql\u001B[0;34m(self, line, cell)\u001B[0m\n",
       "\u001B[1;32m    185\u001B[0m         query_text \u001B[38;5;241m=\u001B[39m sub_query\u001B[38;5;241m.\u001B[39mquery()\n",
       "\u001B[1;32m    186\u001B[0m         sql_directive \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mentry_point\u001B[38;5;241m.\u001B[39mgetSqlDirective(query_text)\n",
       "\u001B[0;32m--> 187\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_sql_directive(sql_directive, i \u001B[38;5;241m==\u001B[39m number_of_sub_queries \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)\n",
       "\u001B[1;32m    188\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    189\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\n",
       "\u001B[1;32m    190\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_FAILED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    191\u001B[0m         exceptionClassName\u001B[38;5;241m=\u001B[39me\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m,\n",
       "\u001B[1;32m    192\u001B[0m         sqlState\u001B[38;5;241m=\u001B[39msafe_call(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgetSqlState\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n",
       "\u001B[1;32m    193\u001B[0m         errorClass\u001B[38;5;241m=\u001B[39msafe_call(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgetCondition\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:206\u001B[0m, in \u001B[0;36mSqlMagic._handle_sql_directive\u001B[0;34m(self, sql_directive, is_last_query)\u001B[0m\n",
       "\u001B[1;32m    204\u001B[0m     query \u001B[38;5;241m=\u001B[39m sql_directive\u001B[38;5;241m.\u001B[39msql()\n",
       "\u001B[1;32m    205\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_register_udf_if_needed(query)\n",
       "\u001B[0;32m--> 206\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_query_request_result(query)\n",
       "\u001B[1;32m    207\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m directive_name \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCreateView\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDropTable\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAlterTable\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCreateTable\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\u001B[1;32m    208\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_query_request_result(sql_directive\u001B[38;5;241m.\u001B[39msql())\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:151\u001B[0m, in \u001B[0;36mSqlMagic._get_query_request_result\u001B[0;34m(self, query)\u001B[0m\n",
       "\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(widget_bindings \u001B[38;5;241m:=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_widget_cache\u001B[38;5;241m.\u001B[39mvalues) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\u001B[1;32m    150\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPARAM_SYNTAX_USAGE\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m--> 151\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspark\u001B[38;5;241m.\u001B[39msql(query, widget_bindings)\n",
       "\u001B[1;32m    152\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/session.py:875\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    872\u001B[0m         _views\u001B[38;5;241m.\u001B[39mappend(SubqueryAlias(df\u001B[38;5;241m.\u001B[39m_plan, name))\n",
       "\u001B[1;32m    874\u001B[0m cmd \u001B[38;5;241m=\u001B[39m SQL(sqlQuery, _args, _named_args, _views)\n",
       "\u001B[0;32m--> 875\u001B[0m data, properties, ei \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(cmd\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client))\n",
       "\u001B[1;32m    876\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msql_command_result\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m properties:\n",
       "\u001B[1;32m    877\u001B[0m     df \u001B[38;5;241m=\u001B[39m DataFrame(CachedRelation(properties[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msql_command_result\u001B[39m\u001B[38;5;124m\"\u001B[39m]), \u001B[38;5;28mself\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1556\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n",
       "\u001B[1;32m   1554\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n",
       "\u001B[1;32m   1555\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n",
       "\u001B[0;32m-> 1556\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n",
       "\u001B[1;32m   1557\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n",
       "\u001B[1;32m   1558\u001B[0m )\n",
       "\u001B[1;32m   1559\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n",
       "\u001B[1;32m   1560\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2059\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n",
       "\u001B[1;32m   2056\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n",
       "\u001B[1;32m   2058\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n",
       "\u001B[0;32m-> 2059\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n",
       "\u001B[1;32m   2060\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n",
       "\u001B[1;32m   2061\u001B[0m     ):\n",
       "\u001B[1;32m   2062\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n",
       "\u001B[1;32m   2063\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2035\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n",
       "\u001B[1;32m   2033\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n",
       "\u001B[1;32m   2034\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 2035\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2355\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2355\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2356\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n",
       "\u001B[1;32m   2357\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2429\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   2431\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2433\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2434\u001B[0m                 info,\n",
       "\u001B[1;32m   2435\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2436\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2437\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2438\u001B[0m                 status_code,\n",
       "\u001B[1;32m   2439\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n",
       "\u001B[1;32m   2442\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2443\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[1;32m   2444\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n",
       "\u001B[1;32m   2445\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2446\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `silver_fact_transactions` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
       "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
       "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 1 pos 9;\n",
       "'DescribeRelation false, [col_name#20603, data_type#20604, comment#20605]\n",
       "+- 'UnresolvedTableOrView [silver_fact_transactions], DESCRIBE TABLE, true, true\n",
       "\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.sql.catalyst.ExtendedAnalysisException\n",
       "\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:343)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:324)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:325)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:324)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:324)\n",
       "\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:324)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:324)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:295)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:557)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:280)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:267)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:263)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:557)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:420)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:420)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:99)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:136)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:92)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:617)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:617)\n",
       "\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:606)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:439)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:721)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:961)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n",
       "\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n",
       "\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n",
       "\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n",
       "\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)\n",
       "\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:961)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1620)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:954)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:951)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:951)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:950)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:938)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:949)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:948)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:421)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:420)\n",
       "\tat scala.util.Try$.apply(Try.scala:217)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n",
       "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:481)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:394)\n",
       "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:153)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1127)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1127)\n",
       "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:145)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$5(SparkSession.scala:867)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:830)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3898)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3725)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3523)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:394)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:873)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$2(SqlExecutionMetrics.scala:199)\n",
       "\tat scala.Option.map(Option.scala:242)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$1(SqlExecutionMetrics.scala:199)\n",
       "\tat scala.util.Try$.apply(Try.scala:217)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.setQueryExecution(SqlExecutionMetrics.scala:199)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:870)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:814)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:234)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:222)\n",
       "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n",
       "\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n",
       "\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n",
       "\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:200)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:200)\n",
       "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n",
       "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n",
       "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)\n",
       "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)\n",
       "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)\n",
       "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `silver_fact_transactions` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 1 pos 9;\n'DescribeRelation false, [col_name#20603, data_type#20604, comment#20605]\n+- 'UnresolvedTableOrView [silver_fact_transactions], DESCRIBE TABLE, true, true\n\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:343)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:325)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:324)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:324)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:295)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:557)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:280)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:267)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:263)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:557)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:420)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:420)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:99)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:136)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:92)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:617)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:617)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:606)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:439)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:721)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:961)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:961)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1620)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:954)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:951)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:951)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:950)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:938)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:949)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:948)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:421)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:420)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:481)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:394)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:153)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1127)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1127)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:145)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$5(SparkSession.scala:867)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:830)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3898)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3725)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3523)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:394)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:873)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$2(SqlExecutionMetrics.scala:199)\n\tat scala.Option.map(Option.scala:242)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$1(SqlExecutionMetrics.scala:199)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.setQueryExecution(SqlExecutionMetrics.scala:199)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:870)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:814)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:234)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:222)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:200)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:200)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)"
       },
       "metadata": {
        "errorSummary": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `silver_fact_transactions` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01\n== SQL (line 1, position 10) ==\nDESCRIBE silver_fact_transactions\n         ^^^^^^^^^^^^^^^^^^^^^^^^\n"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "TABLE_OR_VIEW_NOT_FOUND",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": "== SQL (line 1, position 10) ==\nDESCRIBE silver_fact_transactions\n         ^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "sqlState": "42P01",
        "stackTrace": "org.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:343)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:325)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:324)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:324)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:295)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:557)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:280)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:267)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:263)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:557)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:420)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:420)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:99)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:136)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:92)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:617)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:617)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:606)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:439)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:721)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:961)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:961)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1620)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:954)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:951)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:951)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:950)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:938)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:949)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:948)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:421)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:420)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:481)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:394)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:153)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1127)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1127)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:145)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$5(SparkSession.scala:867)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:830)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3898)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3725)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3523)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:394)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:873)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$2(SqlExecutionMetrics.scala:199)\n\tat scala.Option.map(Option.scala:242)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$1(SqlExecutionMetrics.scala:199)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.setQueryExecution(SqlExecutionMetrics.scala:199)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:870)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:814)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:234)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:222)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:200)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:200)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)",
        "startIndex": 9,
        "stopIndex": 32
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-8907920231679062>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_cell_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msql\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDESCRIBE silver_fact_transactions;\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mSELECT COUNT(*) FROM silver_fact_transactions;\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mSELECT * FROM silver_fact_transactions LIMIT 5;\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2541\u001B[0m, in \u001B[0;36mInteractiveShell.run_cell_magic\u001B[0;34m(self, magic_name, line, cell)\u001B[0m\n\u001B[1;32m   2539\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n\u001B[1;32m   2540\u001B[0m     args \u001B[38;5;241m=\u001B[39m (magic_arg_s, cell)\n\u001B[0;32m-> 2541\u001B[0m     result \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   2543\u001B[0m \u001B[38;5;66;03m# The code below prevents the output from being displayed\u001B[39;00m\n\u001B[1;32m   2544\u001B[0m \u001B[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001B[39;00m\n\u001B[1;32m   2545\u001B[0m \u001B[38;5;66;03m# when the last Python token in the expression is a ';'.\u001B[39;00m\n\u001B[1;32m   2546\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(fn, magic\u001B[38;5;241m.\u001B[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001B[38;5;28;01mFalse\u001B[39;00m):\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:194\u001B[0m, in \u001B[0;36mSqlMagic.sql\u001B[0;34m(self, line, cell)\u001B[0m\n\u001B[1;32m    188\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    189\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\n\u001B[1;32m    190\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_FAILED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    191\u001B[0m         exceptionClassName\u001B[38;5;241m=\u001B[39me\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m,\n\u001B[1;32m    192\u001B[0m         sqlState\u001B[38;5;241m=\u001B[39msafe_call(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgetSqlState\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    193\u001B[0m         errorClass\u001B[38;5;241m=\u001B[39msafe_call(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgetCondition\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m--> 194\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    196\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_SUCCEEDED\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    197\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:187\u001B[0m, in \u001B[0;36mSqlMagic.sql\u001B[0;34m(self, line, cell)\u001B[0m\n\u001B[1;32m    185\u001B[0m         query_text \u001B[38;5;241m=\u001B[39m sub_query\u001B[38;5;241m.\u001B[39mquery()\n\u001B[1;32m    186\u001B[0m         sql_directive \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mentry_point\u001B[38;5;241m.\u001B[39mgetSqlDirective(query_text)\n\u001B[0;32m--> 187\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_sql_directive(sql_directive, i \u001B[38;5;241m==\u001B[39m number_of_sub_queries \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    188\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    189\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\n\u001B[1;32m    190\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_FAILED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    191\u001B[0m         exceptionClassName\u001B[38;5;241m=\u001B[39me\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m,\n\u001B[1;32m    192\u001B[0m         sqlState\u001B[38;5;241m=\u001B[39msafe_call(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgetSqlState\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    193\u001B[0m         errorClass\u001B[38;5;241m=\u001B[39msafe_call(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgetCondition\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:206\u001B[0m, in \u001B[0;36mSqlMagic._handle_sql_directive\u001B[0;34m(self, sql_directive, is_last_query)\u001B[0m\n\u001B[1;32m    204\u001B[0m     query \u001B[38;5;241m=\u001B[39m sql_directive\u001B[38;5;241m.\u001B[39msql()\n\u001B[1;32m    205\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_register_udf_if_needed(query)\n\u001B[0;32m--> 206\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_query_request_result(query)\n\u001B[1;32m    207\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m directive_name \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCreateView\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDropTable\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAlterTable\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCreateTable\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    208\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_query_request_result(sql_directive\u001B[38;5;241m.\u001B[39msql())\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:151\u001B[0m, in \u001B[0;36mSqlMagic._get_query_request_result\u001B[0;34m(self, query)\u001B[0m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(widget_bindings \u001B[38;5;241m:=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_widget_cache\u001B[38;5;241m.\u001B[39mvalues) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    150\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPARAM_SYNTAX_USAGE\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 151\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspark\u001B[38;5;241m.\u001B[39msql(query, widget_bindings)\n\u001B[1;32m    152\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/session.py:875\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m    872\u001B[0m         _views\u001B[38;5;241m.\u001B[39mappend(SubqueryAlias(df\u001B[38;5;241m.\u001B[39m_plan, name))\n\u001B[1;32m    874\u001B[0m cmd \u001B[38;5;241m=\u001B[39m SQL(sqlQuery, _args, _named_args, _views)\n\u001B[0;32m--> 875\u001B[0m data, properties, ei \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(cmd\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client))\n\u001B[1;32m    876\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msql_command_result\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m properties:\n\u001B[1;32m    877\u001B[0m     df \u001B[38;5;241m=\u001B[39m DataFrame(CachedRelation(properties[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msql_command_result\u001B[39m\u001B[38;5;124m\"\u001B[39m]), \u001B[38;5;28mself\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1556\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n\u001B[1;32m   1554\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n\u001B[1;32m   1555\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n\u001B[0;32m-> 1556\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n\u001B[1;32m   1557\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n\u001B[1;32m   1558\u001B[0m )\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n\u001B[1;32m   1560\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2059\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n\u001B[1;32m   2056\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   2058\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 2059\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n\u001B[1;32m   2060\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n\u001B[1;32m   2061\u001B[0m     ):\n\u001B[1;32m   2062\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   2063\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2035\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   2033\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   2034\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 2035\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2355\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2355\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2356\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[1;32m   2357\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2429\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2431\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[0;32m-> 2433\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2434\u001B[0m                 info,\n\u001B[1;32m   2435\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2436\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2437\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2438\u001B[0m                 status_code,\n\u001B[1;32m   2439\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2442\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2443\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[1;32m   2444\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n\u001B[1;32m   2445\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2446\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `silver_fact_transactions` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 1 pos 9;\n'DescribeRelation false, [col_name#20603, data_type#20604, comment#20605]\n+- 'UnresolvedTableOrView [silver_fact_transactions], DESCRIBE TABLE, true, true\n\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:343)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:325)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:324)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:324)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:295)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:557)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:280)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:267)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:263)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:557)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:420)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:420)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:99)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:136)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:92)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:617)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:617)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:606)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:439)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:721)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:961)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:961)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1620)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:954)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:951)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:951)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:950)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:938)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:949)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:948)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:421)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:420)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:481)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:394)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:153)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1127)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1127)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:145)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$5(SparkSession.scala:867)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:830)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3898)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3725)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3523)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:394)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:873)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$2(SqlExecutionMetrics.scala:199)\n\tat scala.Option.map(Option.scala:242)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$1(SqlExecutionMetrics.scala:199)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.setQueryExecution(SqlExecutionMetrics.scala:199)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:870)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:814)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:234)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:222)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:200)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:200)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "DESCRIBE silver_fact_transactions;\n",
    "SELECT COUNT(*) FROM silver_fact_transactions;\n",
    "SELECT * FROM silver_fact_transactions LIMIT 5;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91ea4019-1e6b-4a77-a816-7f6443282a3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"USE CATALOG workspace\")\n",
    "spark.sql(\"USE SCHEMA fraud_detection\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cff8ea86-cccc-4954-b6ce-ce1bbc98cbc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>current_catalog()</th><th>current_schema()</th></tr></thead><tbody><tr><td>workspace</td><td>fraud_detection</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "workspace",
         "fraud_detection"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "current_catalog()",
            "nullable": false,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "current_schema()",
            "nullable": false,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 40
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "current_catalog()",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "current_schema()",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT current_catalog(), current_schema();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "257dc1be-f6e9-4230-87a2-b0d8f586c2cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "silver_fact_transactions_df = (\n",
    "    spark.table(\"bronze_transactions\")\n",
    "    .select(\n",
    "        col(\"id\").alias(\"transaction_id\"),\n",
    "        col(\"step\").cast(\"int\"),\n",
    "        col(\"type\").alias(\"transaction_type\"),\n",
    "        col(\"amount\").cast(\"double\"),\n",
    "        col(\"nameOrig\").alias(\"origin_account\"),\n",
    "        col(\"nameDest\").alias(\"destination_account\"),\n",
    "        col(\"oldBalanceOrig\").cast(\"double\"),\n",
    "        col(\"newBalanceOrig\").cast(\"double\"),\n",
    "        col(\"oldBalanceDest\").cast(\"double\"),\n",
    "        col(\"newBalanceDest\").cast(\"double\"),\n",
    "        col(\"isUnauthorizedOverdraft\").cast(\"int\"),\n",
    "        col(\"ingest_time\")\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fb3236e-33fa-4167-a811-813a3ad30e90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    silver_fact_transactions_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(\"silver_fact_transactions\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a8ad223-1a15-431c-aa96-6b60707a93f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>transaction_id</th><th>step</th><th>transaction_type</th><th>amount</th><th>origin_account</th><th>destination_account</th><th>oldBalanceOrig</th><th>newBalanceOrig</th><th>oldBalanceDest</th><th>newBalanceDest</th><th>isUnauthorizedOverdraft</th><th>ingest_time</th></tr></thead><tbody><tr><td>e87bd2ce-e892-4ff2-b559-1dff52a96e0b</td><td>1</td><td>CASH_IN</td><td>15504.98</td><td>C7252612212</td><td>M4639849776</td><td>183279.61</td><td>198784.59</td><td>0.0</td><td>0.0</td><td>0</td><td>2026-02-08T21:37:25.820Z</td></tr><tr><td>ddcec6e4-aeb5-4165-8a85-5e60304013aa</td><td>1</td><td>CASH_IN</td><td>83938.99</td><td>C7567192866</td><td>M0172269636</td><td>5578.3</td><td>89517.29</td><td>0.0</td><td>0.0</td><td>0</td><td>2026-02-08T21:37:25.820Z</td></tr><tr><td>e13f5635-788a-4617-8bf7-7388acefe360</td><td>1</td><td>CASH_IN</td><td>131805.11</td><td>C8879445092</td><td>M2384302511</td><td>29222.15</td><td>161027.26</td><td>0.0</td><td>0.0</td><td>0</td><td>2026-02-08T21:37:25.820Z</td></tr><tr><td>1402e4cf-9173-4b00-bdac-a6ee620a509b</td><td>1</td><td>CASH_IN</td><td>176931.04</td><td>C7110900402</td><td>M8592581184</td><td>52851.85</td><td>229782.89</td><td>0.0</td><td>0.0</td><td>0</td><td>2026-02-08T21:37:25.820Z</td></tr><tr><td>9b986898-70af-4d30-9f7e-c3b2da193cc9</td><td>1</td><td>CASH_OUT</td><td>268084.67</td><td>CC1229472380</td><td>M0141939366</td><td>268084.67</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>2026-02-08T21:37:25.820Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "e87bd2ce-e892-4ff2-b559-1dff52a96e0b",
         1,
         "CASH_IN",
         15504.98,
         "C7252612212",
         "M4639849776",
         183279.61,
         198784.59,
         0.0,
         0.0,
         0,
         "2026-02-08T21:37:25.820Z"
        ],
        [
         "ddcec6e4-aeb5-4165-8a85-5e60304013aa",
         1,
         "CASH_IN",
         83938.99,
         "C7567192866",
         "M0172269636",
         5578.3,
         89517.29,
         0.0,
         0.0,
         0,
         "2026-02-08T21:37:25.820Z"
        ],
        [
         "e13f5635-788a-4617-8bf7-7388acefe360",
         1,
         "CASH_IN",
         131805.11,
         "C8879445092",
         "M2384302511",
         29222.15,
         161027.26,
         0.0,
         0.0,
         0,
         "2026-02-08T21:37:25.820Z"
        ],
        [
         "1402e4cf-9173-4b00-bdac-a6ee620a509b",
         1,
         "CASH_IN",
         176931.04,
         "C7110900402",
         "M8592581184",
         52851.85,
         229782.89,
         0.0,
         0.0,
         0,
         "2026-02-08T21:37:25.820Z"
        ],
        [
         "9b986898-70af-4d30-9f7e-c3b2da193cc9",
         1,
         "CASH_OUT",
         268084.67,
         "CC1229472380",
         "M0141939366",
         268084.67,
         0.0,
         0.0,
         0.0,
         0,
         "2026-02-08T21:37:25.820Z"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "transaction_id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "step",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "transaction_type",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "amount",
            "nullable": true,
            "type": "double"
           },
           {
            "metadata": {},
            "name": "origin_account",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "destination_account",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "oldBalanceOrig",
            "nullable": true,
            "type": "double"
           },
           {
            "metadata": {},
            "name": "newBalanceOrig",
            "nullable": true,
            "type": "double"
           },
           {
            "metadata": {},
            "name": "oldBalanceDest",
            "nullable": true,
            "type": "double"
           },
           {
            "metadata": {},
            "name": "newBalanceDest",
            "nullable": true,
            "type": "double"
           },
           {
            "metadata": {},
            "name": "isUnauthorizedOverdraft",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "ingest_time",
            "nullable": true,
            "type": "timestamp"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 43
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "transaction_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "step",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "transaction_type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "amount",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "origin_account",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "destination_account",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "oldBalanceOrig",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "newBalanceOrig",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "oldBalanceDest",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "newBalanceDest",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "isUnauthorizedOverdraft",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "ingest_time",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "DESCRIBE silver_fact_transactions;\n",
    "\n",
    "SELECT COUNT(*) FROM silver_fact_transactions;\n",
    "\n",
    "SELECT *\n",
    "FROM silver_fact_transactions\n",
    "LIMIT 5;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c2f0665-54cb-451c-a13d-a5d7975698c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"USE CATALOG workspace\")\n",
    "spark.sql(\"USE SCHEMA fraud_detection\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a94110e-a0b0-4652-aa69-969d7f2d51dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "silver_dim_customers_df = (\n",
    "    spark.table(\"bronze_transactions\")\n",
    "    .select(col(\"nameOrig\").alias(\"customer_id\"))\n",
    "    .union(\n",
    "        spark.table(\"bronze_transactions\")\n",
    "        .select(col(\"nameDest\").alias(\"customer_id\"))\n",
    "    )\n",
    "    .dropna()\n",
    "    .dropDuplicates([\"customer_id\"])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05a3ffd9-7b47-406b-8336-b51b7d28ed57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    silver_dim_customers_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"silver_dim_customers\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38fb3859-cf4a-46eb-a559-92fe31a24c88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th></tr></thead><tbody><tr><td>C7168983926</td></tr><tr><td>C2401457690</td></tr><tr><td>C2317870228</td></tr><tr><td>C1189806313</td></tr><tr><td>CC0189065943</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "C7168983926"
        ],
        [
         "C2401457690"
        ],
        [
         "C2317870228"
        ],
        [
         "C1189806313"
        ],
        [
         "CC0189065943"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "customer_id",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 49
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT COUNT(*) FROM silver_dim_customers;\n",
    "SELECT * FROM silver_dim_customers LIMIT 5;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a64b079-ea10-4f6e-9f9a-734dc0cabf09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_dim_countries_df = (\n",
    "    spark.table(\"bronze_transactions\")\n",
    "    .select(col(\"countryOrig\").alias(\"country_code\"))\n",
    "    .union(\n",
    "        spark.table(\"bronze_transactions\")\n",
    "        .select(col(\"countryDest\").alias(\"country_code\"))\n",
    "    )\n",
    "    .dropna()\n",
    "    .dropDuplicates([\"country_code\"])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b07a5135-4c8d-449f-aa2b-9556c7aa3c3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    silver_dim_countries_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"silver_dim_countries\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32bce279-6f7f-4aa3-936b-f44e1c26fc47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>country_code</th></tr></thead><tbody><tr><td>CAN</td></tr><tr><td>ITA</td></tr><tr><td>ESP</td></tr><tr><td>QAT</td></tr><tr><td>PSE</td></tr><tr><td>PER</td></tr><tr><td>TUR</td></tr><tr><td>RUS</td></tr><tr><td>FRA</td></tr><tr><td>KHM</td></tr><tr><td>PAN</td></tr><tr><td>REU</td></tr><tr><td>BRA</td></tr><tr><td>NGA</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "CAN"
        ],
        [
         "ITA"
        ],
        [
         "ESP"
        ],
        [
         "QAT"
        ],
        [
         "PSE"
        ],
        [
         "PER"
        ],
        [
         "TUR"
        ],
        [
         "RUS"
        ],
        [
         "FRA"
        ],
        [
         "KHM"
        ],
        [
         "PAN"
        ],
        [
         "REU"
        ],
        [
         "BRA"
        ],
        [
         "NGA"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "country_code",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 53
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "country_code",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT COUNT(*) FROM silver_dim_countries;\n",
    "SELECT * FROM silver_dim_countries;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "167629a6-ff3d-498c-8de0-8a3117e105ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_dim_fraud_flags_df = (\n",
    "    spark.table(\"bronze_transactions\")\n",
    "    .select(\n",
    "        col(\"id\").alias(\"transaction_id\"),\n",
    "        col(\"isUnauthorizedOverdraft\").cast(\"int\").alias(\"unauthorized_overdraft_flag\")\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8acc730-c5e3-4db5-8359-a12b8df1e391",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    silver_dim_fraud_flags_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"silver_dim_fraud_flags\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "498cb014-b94a-42f2-a84e-0455df1c60b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>transaction_id</th><th>unauthorized_overdraft_flag</th></tr></thead><tbody><tr><td>90c0e73f-3878-41a7-8964-609e10f2c1b4</td><td>0</td></tr><tr><td>5055ca4b-d16d-4a05-97c7-a9e38fb9fb85</td><td>0</td></tr><tr><td>34db2fa6-1c2b-4e47-919b-fb5f1015276a</td><td>0</td></tr><tr><td>4eba2416-4b83-477d-8aec-fc056f1ccf38</td><td>0</td></tr><tr><td>ef0bb47c-d9c5-4f16-8cfb-d986084e1695</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "90c0e73f-3878-41a7-8964-609e10f2c1b4",
         0
        ],
        [
         "5055ca4b-d16d-4a05-97c7-a9e38fb9fb85",
         0
        ],
        [
         "34db2fa6-1c2b-4e47-919b-fb5f1015276a",
         0
        ],
        [
         "4eba2416-4b83-477d-8aec-fc056f1ccf38",
         0
        ],
        [
         "ef0bb47c-d9c5-4f16-8cfb-d986084e1695",
         0
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "transaction_id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "unauthorized_overdraft_flag",
            "nullable": true,
            "type": "integer"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 57
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "transaction_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "unauthorized_overdraft_flag",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT COUNT(*) FROM silver_dim_fraud_flags;\n",
    "SELECT * FROM silver_dim_fraud_flags LIMIT 5;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a9309e9-0afe-408d-9e18-9a1356e5c888",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 2
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "USE CATALOG workspace;\n",
    "USE SCHEMA fraud_detection;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46494d71-4bdc-4052-9d3c-fdf8a212b181",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_inserted_rows</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "num_affected_rows",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "num_inserted_rows",
            "nullable": true,
            "type": "long"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 3
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num_affected_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_inserted_rows",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE silver_dim_accounts\n",
    "AS\n",
    "SELECT DISTINCT\n",
    "    origin_account AS account_id\n",
    "FROM silver_fact_transactions\n",
    "WHERE origin_account IS NOT NULL\n",
    "\n",
    "UNION\n",
    "\n",
    "SELECT DISTINCT\n",
    "    destination_account AS account_id\n",
    "FROM silver_fact_transactions\n",
    "WHERE destination_account IS NOT NULL;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c38ae55-a29f-46c6-b9c6-ffcb7937ff62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>COUNT(*)</th></tr></thead><tbody><tr><td>89686</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         89686
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "COUNT(*)",
            "nullable": false,
            "type": "long"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 4
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "COUNT(*)",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT COUNT(*) FROM silver_dim_accounts;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb362080-ead2-4c97-98be-2c6657c6273c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>account_id</th></tr></thead><tbody><tr><td>C2317870228</td></tr><tr><td>C4603372195</td></tr><tr><td>C1189806313</td></tr><tr><td>C9679880050</td></tr><tr><td>C2796449758</td></tr><tr><td>C7168983926</td></tr><tr><td>C2401457690</td></tr><tr><td>CC9915820871</td></tr><tr><td>CC0786317222</td></tr><tr><td>CC6000864049</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "C2317870228"
        ],
        [
         "C4603372195"
        ],
        [
         "C1189806313"
        ],
        [
         "C9679880050"
        ],
        [
         "C2796449758"
        ],
        [
         "C7168983926"
        ],
        [
         "C2401457690"
        ],
        [
         "CC9915820871"
        ],
        [
         "CC0786317222"
        ],
        [
         "CC6000864049"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "account_id",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 5
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "account_id",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT * FROM silver_dim_accounts LIMIT 10;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7a22cb1-7f48-485d-a231-e6af8f62f85b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>col_name</th><th>data_type</th><th>comment</th></tr></thead><tbody><tr><td>account_id</td><td>string</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "account_id",
         "string",
         null
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "col_name",
            "nullable": false,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "data_type",
            "nullable": false,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "comment",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 7
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "col_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "data_type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "comment",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "DESCRIBE silver_dim_accounts;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5423c356-66c3-453f-a440-0817c52155e5",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770592350176}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>col_name</th><th>data_type</th><th>comment</th></tr></thead><tbody><tr><td>customer_id</td><td>string</td><td>null</td></tr><tr><td>country_code</td><td>string</td><td>null</td></tr><tr><td>ingest_time</td><td>timestamp</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "customer_id",
         "string",
         null
        ],
        [
         "country_code",
         "string",
         null
        ],
        [
         "ingest_time",
         "timestamp",
         null
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "col_name",
            "nullable": false,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "data_type",
            "nullable": false,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "comment",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 9
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "col_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "data_type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "comment",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "DESCRIBE bronze_customers;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ccc751f-c084-4179-b277-87246993388b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6027070993498666,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "fraud_detection_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}